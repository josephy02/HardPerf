# Large Diffusion Model Configuration
# High-resolution image generation with U-Net architecture
# Data parallelism across 8x NVIDIA H200 GPUs

model:
  type: diffusion_model
  size_billions: 12.0  # Large diffusion model (Stable Diffusion XL scale)
  batch_size: 16  # 2 per GPU (high-res images are memory intensive)
  sequence_length: 4096  # 1024x1024 images flattened
  hidden_dim: 2048
  framework: pytorch

hardware:
  gpu_type: "NVIDIA H200"

# Single-node cluster optimal for diffusion training
cluster:
  num_gpus: 8
  interconnect: "nvlink_5"
  nodes: 1
  gpus_per_node: 8

# Data parallelism for diffusion models
distribution:
  strategy: "data_parallel"
  data_parallel: 8
  tensor_parallel: 1
  pipeline_parallel: 1

analysis:
  enable_bottlenecks: true
  basic_mode: false

output:
  file: "results/diffusion_12b_high_res.json"
  format: json
  create_dir: true

advanced:
  optimizer: AdamW
  precision: FP16
  activation_checkpointing: true  # Essential for U-Net skip connections
  training_hours: 720.0  # 1 month of training
  checkpoint_frequency: 12.0

model_specific:
  transformer:
    num_attention_heads: 16
    ffn_expansion_factor: 4
    use_rope: false  # Diffusion models use different positional encoding
    multi_query_attention: false

# Diffusion-specific configuration
diffusion_config:
  num_timesteps: 1000  # Standard DDPM timesteps
  noise_schedule: "cosine"  # Improved noise scheduling
  unet_channels: [320, 640, 1280, 1280]  # U-Net channel progression
  attention_resolutions: [32, 16, 8]  # Where to apply attention
  
# U-Net Architecture Settings
unet_config:
  in_channels: 4  # Latent space channels
  out_channels: 4
  model_channels: 320
  num_res_blocks: 2
  channel_mult: [1, 2, 4, 4]
  
experimental:
  enable_experimental: true
  memory_efficiency_factor: 0.80  # U-Net has complex memory patterns

# Training optimizations for diffusion
training_optimization:
  gradient_clipping: 1.0  # Important for diffusion stability
  ema_decay: 0.9999  # Exponential moving average for better results
  mixed_precision_loss_scaling: 128
  
# Expected Performance:
# - Memory per GPU: ~110-130 GB (fits in 141 GB H200)
# - Training speed: Good with data parallelism
# - Image quality: 1024x1024 high-resolution generation
# - Use case: Stable Diffusion XL, DALL-E style image generation