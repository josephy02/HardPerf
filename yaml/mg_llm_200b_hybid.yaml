# Advanced Hybrid Parallelism Configuration
# 200B+ Parameter Model with Combined Strategies
# Tensor + Pipeline + Data Parallelism on 64x NVIDIA B200

model:
  type: transformer_decoder
  size_billions: 200.0  # Massive scale language model
  batch_size: 256  # Large global batch size
  sequence_length: 4096
  hidden_dim: 16384  # Very large hidden dimension
  framework: pytorch

hardware:
  gpu_type: "NVIDIA B200"

# Large multi-node cluster
cluster:
  num_gpus: 64
  interconnect: "nvswitch_3"  # Required for this scale
  nodes: 8
  gpus_per_node: 8

# Hybrid parallelism: All three strategies combined
distribution:
  strategy: "hybrid"
  data_parallel: 4      # 4 data-parallel groups
  tensor_parallel: 4    # 4-way tensor parallelism within each stage
  pipeline_parallel: 4  # 4-stage pipeline

analysis:
  enable_bottlenecks: true
  basic_mode: false

output:
  file: "results/llm_200b_hybrid_parallelism.json"
  format: json
  create_dir: true

advanced:
  optimizer: AdamW
  precision: FP16
  activation_checkpointing: true
  training_hours: 8760.0  # 1 year of training
  checkpoint_frequency: 72.0  # Every 3 days

model_specific:
  transformer:
    num_attention_heads: 128  # 16384 / 128 = 128 heads
    ffn_expansion_factor: 4
    use_rope: true
    multi_query_attention: true  # Efficiency for massive scale

# Advanced Hybrid Configuration
hybrid_config:
  # Pipeline settings
  pipeline_stages: 4
  micro_batch_size: 1  # Small micro-batches for memory
  virtual_pipeline_stages: 2
  
  # Tensor parallel settings
  sequence_parallel: true
  expert_parallel: false  # Not using MoE in this config
  
  # Data parallel settings
  gradient_synchronization: "hierarchical"  # Multi-level reduction

# Extreme Scale Optimizations
extreme_scale_optimization:
  gradient_compression: true
  dynamic_loss_scaling: true
  memory_efficient_attention: true
  flash_attention: true
  
  # Multi-node specific
  hierarchical_all_reduce: true
  bucket_gradients: true
  overlap_all_operations: true

# Resource Management
resource_management:
  memory_pool_size: "180GB"  # Pre-allocate memory pool
  cpu_offload_optimizer: true  # Offload optimizer to CPU when possible
  nvme_offload: false  # Keep on GPU for this config
  
experimental:
  enable_experimental: true
  memory_efficiency_factor: 0.75  # Complex hybrid has more overhead

# Monitoring and Debugging
monitoring:
  profile_memory: true
  profile_communication: true
  track_gradient_norms: true
  log_pipeline_bubbles: true

# Expected Performance:
# - Memory per GPU: ~150-170 GB (fits in 192 GB B200)
# - Training complexity: Extremely high
# - Communication overhead: ~40-50% (complex hybrid)
# - Use case: Research into frontier models (GPT-5+ scale)
# - Requirements: Expert distributed training team