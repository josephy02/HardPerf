# Large Language Model Configuration: 175B Parameter Model (GPT-3 scale)
# Pipeline Parallelism Strategy for Massive Models
# Multi-node setup with 32x NVIDIA H200 GPUs

model:
  type: transformer_decoder
  size_billions: 175.0
  batch_size: 128  # Large global batch for stability
  sequence_length: 2048
  hidden_dim: 12288  # GPT-3 scale
  framework: pytorch

hardware:
  gpu_type: "NVIDIA H200"

# Multi-node cluster for massive scale
cluster:
  num_gpus: 32
  interconnect: "nvswitch_3"
  nodes: 4
  gpus_per_node: 8

# Pipeline parallelism with data parallelism
distribution:
  strategy: "pipeline_parallel"
  data_parallel: 4      # 4 data-parallel groups
  tensor_parallel: 1
  pipeline_parallel: 8  # 8-stage pipeline

analysis:
  enable_bottlenecks: true
  basic_mode: false

output:
  file: "results/llm_175b_pipeline_parallel.json"
  format: json
  create_dir: true

advanced:
  optimizer: AdamW
  precision: FP16
  activation_checkpointing: true
  training_hours: 4320.0  # 6 months of training
  checkpoint_frequency: 48.0  # Every 2 days

model_specific:
  transformer:
    num_attention_heads: 96  # 12288 / 128 = 96 heads
    ffn_expansion_factor: 4
    use_rope: true
    multi_query_attention: false

# Pipeline Parallelism Configuration
pipeline_config:
  num_stages: 8  # 8 pipeline stages
  micro_batch_size: 4  # 4 micro-batches per stage
  gradient_accumulation_steps: 8  # For large effective batch
  
# Advanced Pipeline Settings
pipeline_optimization:
  interleaved_schedule: true  # Better bubble efficiency
  virtual_pipeline_stages: 2  # Reduce pipeline bubbles
  async_communication: true
  
experimental:
  enable_experimental: true
  memory_efficiency_factor: 0.80  # Pipeline has overhead

# Multi-node Communication
multi_node_config:
  gradient_compression: true
  hierarchical_all_reduce: true  # Optimize multi-node gradients
  bandwidth_scaling: 0.75  # Account for inter-node overhead

# Expected Performance:
# - Memory per GPU: ~100-120 GB (fits in 141 GB H200)
# - Pipeline efficiency: ~60-70% (due to bubbles)
# - Training speed: Excellent for massive models
# - Use case: GPT-3/GPT-4 scale models requiring maximum parameter count
